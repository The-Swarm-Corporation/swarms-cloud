### Deploy With LM_Deploy ######## 
envs:
  MODEL_NAME: Qwen/Qwen-VL
  HF_HUB_ENABLE_HF_TRANSFER: True

resources:
  # accelerators: {L4:4, A100:4, A100:8, A100-80GB:2, A100-80GB:4, A100-80GB:8} ## Large models
  accelerators: [L4, A10g, A100, A100, A100-80GB, T4, M60] ## Small models
  # cpus: 32+
  memory: 32+
  # use_spot: True
  # disk_size: 512  # Ensure model checkpoints (~246GB) can fit.
  # disk_tier: best
  ports: 8080  # Expose model to internet traffic.
  ports: 8000  # Expose Prometheus to internet traffic.

service:
  readiness_probe:
    path: /v1/chat/completions
    post_data:
      model: $MODEL_NAME
      messages:
        - role: user
          content: Hello! What is your name?
      max_tokens: 1
  readiness_probe: /v1/models
  replica_policy:
    min_replicas: 0
    max_replicas: 10
    target_qps_per_replica: 2.5
    upscale_delay_seconds: 300
    downscale_delay_seconds: 1200

setup: |

  pip install hf_transfer && \
  pip install prometheus_client && \
  pip install lmdeploy && \

  # Setup Profiling
  git clone --depth=1 https://github.com/ZackBradshaw/lmdeploy && \
  cd lmdeploy && \
  pip install -r requirements.txt && \
  cd benchmark
  python3 profile_generation.py $MODEL_NAME && \
  wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json && \
  python3 profile_restful_api.py http://0.0.0.0:8000 internlm/internlm-7b ./ShareGPT_V3_unfiltered_cleaned_split.json 

run: | #### LM_Deploy with lmdeploy https://lmdeploy.readthedocs.io/en/latest/get_started.html#installation

  # Serve With Docker
  docker run --runtime nvidia --gpus all \
      -v ~/.cache/huggingface:/root/.cache/huggingface \
      --env "HUGGING_FACE_HUB_TOKEN={$HUGGING_FACE_HUB_TOKEN}"\
      -p 8080:8080 \
      --ipc=host \
      openmmlab/lmdeploy:latest \
    pip install lmdeploy[vision] \
    lmdeploy serve api_server $MODEL_NAME --server-port 8080

