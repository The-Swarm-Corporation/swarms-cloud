envs:
  MODEL_NAME: meta-llama/Meta-Llama-3-8B
  HF_HUB_ENABLE_HF_TRANSFER: True


# service.yaml
service:
  # An actual request for readiness probe.
  readiness_probe:
    path: /v1/chat/completions
    post_data:
      model: $MODEL_NAME
      messages:
        - role: user
          content: Hello! What is your name?
      max_tokens: 1
  readiness_probe: /health
  readiness_probe: /v1/models
  replica_policy:
    min_replicas: 1
    max_replicas: 10
    target_qps_per_replica: 2.5
    upscale_delay_seconds: 300
    downscale_delay_seconds: 1200

# # # Advanced Kubernetes configurations (optional).
# kubernetes:
#   # The networking mode for accessing SSH jump pod (optional).
#   #
#   # This must be either: 'nodeport' or 'portforward'. If not specified,
#   # defaults to 'portforward'.

#   #
#   # nodeport: Exposes the jump pod SSH service on a static port number on each
#   # Node, allowing external access to using <NodeIP>:<NodePort>. Using this
#   # mode requires opening multiple ports on nodes in the Kubernetes cluster.
#   #
#   # portforward: Uses `kubectl port-forward` to create a tunnel and directly
#   # access the jump pod SSH service in the Kubernetes cluster. Does not
#   # require opening ports the cluster nodes and is more secure. 'portforward'
#   # is used as default if 'networking' is not specified.
#   networking: portforward

#   # The mode to use for opening ports on Kubernetes
#   #
#   # This must be either: 'ingress' or 'loadbalancer'. If not specified,
#   # defaults to 'loadbalancer'.
#   #
#   # loadbalancer: Creates services of type `LoadBalancer` to expose ports.
#   # See https://skypilot.readthedocs.io/en/latest/reference/kubernetes/kubernetes-setup.html#loadbalancer-service.
#   # This mode is supported out of the box on most cloud managed Kubernetes
#   # environments (e.g., GKE, EKS).
#   #
#   # ingress: Creates an ingress and a ClusterIP service for each port opened.
#   # Requires an Nginx ingress controller to be configured on the Kubernetes cluster.
#   # Refer to https://skypilot.readthedocs.io/en/latest/reference/kubernetes/kubernetes-setup.html#nginx-ingress
#   # for details on deploying the NGINX ingress controller.
#   ports: loadbalancer

#   # Attach custom metadata to Kubernetes objects created by SkyPilot
#   #
#   # Uses the same schema as Kubernetes metadata object: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#objectmeta-v1-meta
#   #
#   # Since metadata is applied to all all objects created by SkyPilot,
#   # specifying 'name' and 'namespace' fields here is not allowed.
#   # custom_metadata:
#   #   labels:
#   #     mylabel: myvalue
#   #   annotations:
#   #     myannotation: myvalue

#   # Additional fields to override the pod fields used by SkyPilot (optional)
#   #
#   # Any key:value pairs added here would get added to the pod spec used to
#   # create SkyPilot pods. The schema follows the same schema for a Pod object
#   # in the Kubernetes API:
#   # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.26/#pod-v1-core
#   #
#   # # Some example use cases are shown below. All fields are optional.
#   # pod_config:
#   #   spec:
#   #     runtimeClassName: nvidia    # Custom runtimeClassName for GPU pods.
#   #     containers:
#   #       - env:                # Custom environment variables for the pod, e.g., for proxy
#   #         - name: HTTP_PROXY
#   #           value: http://proxy-host:3128
#   #         volumeMounts:       # Custom volume mounts for the pod
#   #           - mountPath: /foo
#   #             name: swarms
#   #             readOnly: true
#   #     volumes:
#   #       - name: swarms
#   #         hostPath:
#   #           path: /tmp
#   #           type: Directory
#   #       - name: swarms          # Use this to modify the /dev/shm volume mounted by SkyPilot
#   #         emptyDir:
#   #           medium: Memory
#   #           sizeLimit: 3Gi    # Set a size limit for the /dev/shm volume


# Fields below describe each replica.
resources:
  accelerators: {T4:1, L4:4, A10g:8, A100:4, A100:8, A100-80GB:2, A100-80GB:4, A100-80GB:8}
  # cpus: 32+
  # memory: 512+
  # use_spot: True
  # disk_size: 512  # Ensure model checkpoints (~246GB) can fit.
  # disk_tier: best
  ports: 8080  # Expose to internet traffic.

# workdir: ~/swarms-cloud/servers/cogvlm

setup: |
  docker pull vllm/vllm-openai:latest

run: |
  docker run --runtime nvidia --gpus all \
    -v ~/.cache/huggingface:/root/.cache/huggingface \
    --env "HUGGING_FACE_HUB_TOKEN=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" \
    -p 8000:8000 \
    --ipc=host \
    vllm/vllm-openai:latest \
    --model $MODEL_NAME