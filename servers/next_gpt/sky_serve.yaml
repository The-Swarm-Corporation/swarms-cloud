envs:
  MODEL_NAME: microsoft/Phi-3-vision-128k-instruct
  MODEL_ARCH: Phi-3-vision
  HUGGING_FACE_HUB_TOKEN: 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
  MODEL_PORT: 8080

resources:
  # accelerators: {L4:4, A100:4, A100:8, A100-80GB:2, A100-80GB:4, A100-80GB:8} ## Large models
  accelerators: [A10g, A100, A100, A100-80GB, T4, M60] ## Small models
  # cpus: 32+
  memory: 32+
  use_spot: True
  disk_size: 512  # Ensure model checkpoints (~246GB) can fit.
  # disk_tier: best
  ports: 8080  # Expose to internet traffic.

service:
  readiness_probe:
    path: /v1/chat/completions
    post_data:
      model: $MODEL_NAME
      messages:
        - role: user
          content: Hello! What is your name?
      max_tokens: 1
  readiness_probe: /v1/models

  # Replica Policy
  replica_policy:
    min_replicas: 3  # Minimum number of replicas
    max_replicas: 100  # Maximum number of replicas
    target_qps_per_replica: 2.5  # Target queries per second per replica
    upscale_delay_seconds: 40  # Delay before upscaling replicas
    downscale_delay_seconds: 20  # Delay before downscaling replicas


setup: |
  #!/bin/bash

  # Step 1: Environment Preparation
  conda env create -n nextgpt python=3.8
  conda activate nextgpt

  # Install CUDA 11.6
  conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.6 -c pytorch -c nvidia

  # Clone NExT-GPT repository
  git clone https://github.com/NExT-GPT/NExT-GPT.git
  cd NExT-GPT

  # Install required packages
  pip install -r requirements.txt

  # Step 2: Preparing Pre-trained Checkpoints
  mkdir -p ckpt/pretrained_ckpt/imagebind_ckpt
  wget -O ckpt/pretrained_ckpt/imagebind_ckpt/huge.pth https://dl.fbaipublicfiles.com/imagebind/imagebind_huge.pth

  # Prepare Vicuna Checkpoint
  pip install git+https://github.com/lm-sys/FastChat.git@v0.1.10

  # Replace the paths below with actual paths to your downloaded LLaMA and Vicuna weights
  LLaMA_PATH=/path/to/llama_weights
  VICUNA_PATH=/path/to/delta_vicuna_weights

  python -m fastchat.model.apply_delta --base $LLaMA_PATH --target ckpt/pretrained_ckpt/vicuna_ckpt/ --delta $VICUNA_PATH

  # Step 3: Preparing Dataset

  # CC3M Dataset
  pip install img2dataset

  img2dataset --url_list Train_GCC-training.tsv --input_format "tsv" --url_col "url" --caption_col "caption" --output_format webdataset --output_folder data/T-X_pair_data/cc3m --processes_count 16 --thread_count 64 --image_size 256 --enable_wandb True

  # Verify and organize the dataset into JSON format (manual step)
  # Ensure cc3m.json and images are placed correctly in data/T-X_pair_data/cc3m

  # WebVid Dataset
  wget -nc http://www.robots.ox.ac.uk/~maxbain/webvid/results_2M_train.csv

  video2dataset --url_list="results_2M_train.csv" --input_format="csv" --output_format="webdataset" --output_folder="data/T-X_pair_data/webvid" --url_col="contentUrl" --caption_col="name" --save_additional_columns='[videoid,page_idx,page_dir,duration]' --enable_wandb=True

  # Verify and organize the dataset into JSON format (manual step)
  # Ensure webvid.json and videos are placed correctly in data/T-X_pair_data/webvid

  # AudioCap Dataset
  sudo apt install ffmpeg
  pip install audiocaps-download

  # Download CSV from https://audiocaps.github.io/
  # Replace the path below with the actual path to your downloaded CSV
  AUDIOCAPS_CSV_PATH=/path/to/audiocaps.csv

  python -m audiocaps_download.Downloader --root_path='data/T-X_pair_data/audiocap/' --n_jobs=16 --csv_path=$AUDIOCAPS_CSV_PATH --format='wav'

  # Verify and organize the dataset into JSON format (manual step)
  # Ensure audiocap.json and audios are placed correctly in data/T-X_pair_data/audiocap

  # Step 4: Precomputing Embeddings
  cd code
  python process_embeddings.py ../data/T-X_pair_data/cc3m/cc3m.json image ../data/embed/ runwayml/stable-diffusion-v1-5


run: | 
  # Step 5: Training NExT-GPT
  # Stage 1: Encoding-side LLM-centric Multimodal Alignment
  deepspeed --include localhost:0 --master_addr 127.0.0.1 --master_port 28459 train.py \
      --model nextgpt \
      --stage 1 \
      --save_path ../ckpt/delta_ckpt/nextgpt/7b_tiva_v0/ \
      --log_path ../ckpt/delta_ckpt/nextgpt/7b_tiva_v0/log/

  # # Stage 2: Decoding-side Instruction-following Alignment
  # deepspeed --include localhost:0 --master_addr 127.0.0.1 --master_port 28459 train.py \
  #     --model nextgpt \
  #     --stage 2 \
  #     --save_path ../ckpt/delta_ckpt/nextgpt/7b_tiva_v0/ \
  #     --log_path ../ckpt/delta_ckpt/nextgpt/7b_tiva_v0/log/

  # # Stage 3: Instruction Tuning
  # deepspeed --include localhost:0 --master_addr 127.0.0.1 --master_port 28459 train.py \
  #     --model nextgpt \
  #     --stage 3 \
  #     --save_path ../ckpt/delta_ckpt/nextgpt/7b_tiva_v0/ \
  #     --log_path ../ckpt/delta_ckpt/nextgpt/7b_tiva_v0/log/
