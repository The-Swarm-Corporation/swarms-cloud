###### Llava served with vllm openai enpoint ######## 
envs:
  MODEL_NAME: cjpais/llava-v1.6-34B-gguf/llava-v1.6-34b.Q4_K_M.gguf
  # MODEL_NAME: llava-hf/llava-v1.6-mistral-7b-hf
  HF_HUB_ENABLE_HF_TRANSFER: True

resources:
  # accelerators: {L4:4, A100:4, A100:8, A100-80GB:2, A100-80GB:4, A100-80GB:8} ## Large models
  # accelerators: {L4, A100, A100, A100-80GB, T4, M60,} ## Small models
  # cpus: 32+
  memory: 32+
  # use_spot: True
  # disk_size: 512  # Ensure model checkpoints (~246GB) can fit.
  # disk_tier: best
  ports: 8080  # Expose to internet traffic.

setup: |
  conda create -n vllm python=3.9 -y
  conda activate vllm
  pip install -U git+https://github.com/DarkLight1337/vllm-rocm.git@openai-vision-api

run: |
  conda activate vllm 
    python -m vllm.entrypoints.openai.api_server \
      --model $MODEL_NAME
      --image-input-type pixel_values
      --image-token-id 32000
      --image-input-shape 1,3,336,336
      --image-feature-size 658
      --tensor-parallel-size $SKYPILOT_NUM_GPUS_PER_NODE \
      --host 0.0.0.0 --port 8080 \
      --max-model-len 32768

# Fields below describe each replica.
# service.yaml
service:
  # An actual request for readiness probe.
  readiness_probe:
    path: /v1/chat/completions
    post_data:
      model: $MODEL_NAME
      messages:
        - role: user
          content: Hello! What is your name?
      max_tokens: 1
  readiness_probe: /health
  readiness_probe: /v1/models
  replica_policy:
    min_replicas: 1
    max_replicas: 10
    target_qps_per_replica: 2.5
    upscale_delay_seconds: 300
    downscale_delay_seconds: 1200
    