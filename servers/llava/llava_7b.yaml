# envs:  # Uncomment and define environment variables if needed

resources:
  # accelerators: {L4:4, A100:4, A100:8, A100-80GB:2, A100-80GB:4, A100-80GB:8} ## Large models
  accelerators: [L4, A10g, A100, A100, A100-80GB, T4, M60] ## Small models
  # cpus: 32+
  memory: 32+
  # use_spot: True
  # disk_size: 512  # Ensure model checkpoints (~246GB) can fit.
  # disk_tier: best
  ports: 8080  # Expose to internet traffic.

service:
  readiness_probe:
    path: /v1/chat/completions
    post_data:
      model: $MODEL_NAME
      messages:
        - role: user
          content: Hello! What is your name?
      max_tokens: 1
  readiness_probe: /v1/models/  # Remove or combine with the above probe
  replica_policy:
    min_replicas: 1
    max_replicas: 10
    target_qps_per_replica: 5
    upscale_delay_seconds: 300
    downscale_delay_seconds: 1200

setup: |
  # Download the model
  wget -O llava-v1.5-7b-q4.llamafile https://huggingface.co/jartine/llava-v1.5-7B-GGUF/resolve/main/llava-v1.5-7b-q4.llamafile?download=true
  chmod +x llava-v1.5-7b-q4.llamafile

  # Setup cloudwatch agent for gpu usage
  wget https://s3.us-west-2.amazonaws.com/amazoncloudwatch-agent-us-west-2/ubuntu/arm64/latest/amazon-cloudwatch-agent.deb
  sudo systemctl enable dlami-cloudwatch-agent@all
  sudo systemctl start dlami-cloudwatch-agent@all

run: | 
  ./llava-v1.5-7b-q4.llamafile -ngl 9999 --nobrowser
